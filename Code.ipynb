#Cryptocurrency Transaction Network Analysis for Anomaly Detection

#Packages

import numpy as np
import pandas as pd
import networkx as nx
import matplotlib.pyplot as pl
import matplotlib.pyplot as plt
from itertools import product


#Reading CSV Data

df = pd.read_csv('eth-dataset.csv')

df.head()

#Data Preprocessing and Visualisation


def hex_to_int(hex_string):
    return int(hex_string, 16)

df['block_hash'] = df['block_hash'].apply(hex_to_int)

df.head()



G = nx.Graph()
for index, row in df.iterrows():
    from_addr = row['from_address']
    to_addr = row['to_address']
    value = row['value']

    G.add_node(from_addr)
    G.add_node(to_addr)

    G.add_edge(from_addr, to_addr, value=value)


pos = nx.spring_layout(G, seed=4)
nx.draw(G, pos, with_labels=False, node_size=2)
plt.title("Cryptocurrency Transaction Network")
plt.show()


df


#Importing sklearn packages


from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.neighbors import NearestNeighbors
from sklearn.model_selection import GridSearchCV


#Implementing the models

#Kmeans


features = df[['value', 'gas', 'gas_price']]

scaler = StandardScaler()
features_standardized = scaler.fit_transform(features)

n_clusters_range = [3, 4, 5, 6, 7]
alpha_values = [0.2, 0.5, 0.8]

best_n_clusters = None
best_alpha = None
best_combined_score = float('-inf')

for n_clusters, alpha in product(n_clusters_range, alpha_values):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    df['cluster'] = kmeans.fit_predict(features_standardized)

    df['distance_to_centroid'] = kmeans.transform(features_standardized).min(axis=1)

    anomaly_threshold = df['distance_to_centroid'].quantile(alpha)

    anomalies = df[df['distance_to_centroid'] > anomaly_threshold]
    silhouette = silhouette_score(features_standardized, df['cluster'])

    davies_bouldin = davies_bouldin_score(features_standardized, df['cluster'])

    combined_score = silhouette - davies_bouldin

    print(f"Parameters: n_clusters={n_clusters}, alpha={alpha}")
    print(f"Silhouette Score: {silhouette:.2f}")
    print(f"Davies-Bouldin Index: {davies_bouldin:.2f}")
    print(f"Combined Score: {combined_score:.2f}\n")
    if combined_score > best_combined_score:
        best_n_clusters = 5
        best_alpha = 0.98
        best_combined_score = combined_score

print("Best Parameters:")
print(f"n_clusters={best_n_clusters}, alpha={best_alpha}")
print(f"Best Combined Score: {best_combined_score:.2f}")



features = df[['value', 'gas', 'gas_price']]
n_clusters = 5
#sqrt of n /2 - found in a research paper -multiview cluster survey paper
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
df['cluster'] = kmeans.fit_predict(features)

df['distance_to_centroid'] = kmeans.transform(features).min(axis=1)

anomaly_threshold = df['distance_to_centroid'].quantile(0.98)

anomalies = df[df['distance_to_centroid'] > anomaly_threshold]

# Silhouette Score
silhouette = silhouette_score(features, df['cluster'])
print(f"Silhouette Score: {silhouette:.2f}")

# Davies-Bouldin Index
davies_bouldin = davies_bouldin_score(features, df['cluster'])
print(f"Davies-Bouldin Index: {davies_bouldin:.2f}")


#kmeans Visualisation
plt.scatter(df['value'], df['gas'], c=df['cluster'], cmap='viridis')
plt.colorbar()
plt.title('K-Means Clustering with Anomalies')
plt.xlabel('Value')
plt.ylabel('Gas')
plt.show()


print("Anomalies:")
print(anomalies)


#InDegree OutDegree

#Transaction Received is Indegree
#Transaction Sent is OutDegree





G = nx.DiGraph()

for index, row in df.iterrows():
    from_addr = row['from_address']
    to_addr = row['to_address']
    value = row['value']

    G.add_edge(from_addr, to_addr, value=value)



in_degrees = G.in_degree()
out_degrees = G.out_degree()

print("In-Degrees:")
print(dict(in_degrees))
print("Out-Degrees:")
print(dict(out_degrees))




#In degree Out degree Visualisation
plt.figure(figsize=(10, 10))
pos = nx.spring_layout(G)
nx.draw(G, pos, with_labels=False, node_size=10)
plt.title("Cryptocurrency Transaction Graph")
plt.axis('off')
plt.show()




G = nx.DiGraph()

for index, row in df.iterrows():
    from_addr = row['from_address']
    to_addr = row['to_address']
    value = row['value']

    G.add_edge(from_addr, to_addr, value=value)

in_degrees = dict(G.in_degree())
out_degrees = dict(G.out_degree())

high_degree_threshold = np.mean(list(in_degrees.values())) + 2 * np.std(list(in_degrees.values()))
low_degree_threshold = np.mean(list(in_degrees.values())) - 2 * np.std(list(in_degrees.values()))

high_degree_receiving_addresses = [node for node, degree in in_degrees.items() if degree > high_degree_threshold]
low_degree_receiving_addresses = [node for node, degree in in_degrees.items() if degree < low_degree_threshold]
high_degree_sending_addresses = [node for node, degree in out_degrees.items() if degree > high_degree_threshold]
low_degree_sending_addresses = [node for node, degree in out_degrees.items() if degree < low_degree_threshold]

anomaly_nodes = set(high_degree_receiving_addresses + low_degree_receiving_addresses + high_degree_sending_addresses + low_degree_sending_addresses)

pos = nx.spring_layout(G)
node_colors = ['r' if node in anomaly_nodes else 'b' for node in G.nodes()]

red_addresses = [address for address in anomaly_nodes if address in G.nodes() and node_colors[list(G.nodes()).index(address)] == 'r']

plt.figure(figsize=(10, 10))
nx.draw(G, pos, with_labels=False, node_size=10, node_color=node_colors)

labels = {node: node if node in red_addresses else '' for node in G.nodes()}
nx.draw_networkx_labels(G, pos, labels=labels, font_color='black', font_size=8)

plt.title("Cryptocurrency Transaction Graph with Anomalies")
plt.axis('off')
plt.show()


#Local Outlier Factor (LOF)



metrics = ['euclidean', 'manhattan']

param_grid = {
    'n_neighbors': [5, 10, 15],
    'contamination': [0.01, 0.05, 0.1],
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']
}

best_params = None
best_score = -np.inf

for metric in metrics:
    for n_neighbors in param_grid['n_neighbors']:
        for contamination in param_grid['contamination']:
            for algorithm in param_grid['algorithm']:
                lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination, algorithm=algorithm, metric=metric)
                y_pred = lof.fit_predict(X)
                silhouette = silhouette_score(X, y_pred)
                davies_bouldin = davies_bouldin_score(X, y_pred)

                combined_score = silhouette - davies_bouldin

                print(f"Parameters: metric={metric}, n_neighbors={n_neighbors}, contamination={contamination}, algorithm={algorithm}")
                print(f"Silhouette Score: {silhouette:.4f}, Davies-Bouldin Index: {davies_bouldin:.4f}")
                print(f"Combined Score: {combined_score:.4f}\n")

                if combined_score > best_score:
                    best_score = combined_score
                    best_params = {'metric': metric, 'n_neighbors': n_neighbors, 'contamination': contamination, 'algorithm': algorithm}

print("Best parameters found:", best_params)




X = df[['value']].values
lof = LocalOutlierFactor(n_neighbors=5, contamination=0.01, metric='euclidean', algorithm='brute')
y_pred = lof.fit_predict(X)
anomalies = df[y_pred == -1]
print("Anomalies:")
print(anomalies)

df['lof_cluster'] = y_pred

silhouette_lof = silhouette_score(X, df['lof_cluster'])
davies_bouldin_lof = davies_bouldin_score(X, df['lof_cluster'])

print(f"Silhouette Score (LOF): {silhouette_lof:.2f}")
print(f"Davies-Bouldin Index (LOF): {davies_bouldin_lof:.2f}")


#LOF Visualisation
plt.scatter(X[:, 0], y_pred, c=y_pred, cmap='viridis')
plt.title('Anomalies Detected by LOF')
plt.xlabel('Value')
plt.ylabel('LOF Score')
plt.show()



#DBSCAN (Density-Based Spatial Clustering of Applications with Noise)



#DBSCAN
feature_columns = ['value', 'gas', 'gas_price']

features = df[feature_columns]
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)



eps_values = np.arange(0.1, 1.0, 0.1)
min_samples_values = [3, 5, 10, 20]

best_eps = None
best_min_samples = None
best_silhouette_score = -1

for eps in eps_values:
    for min_samples in min_samples_values:
        # Fit DBSCAN with current hyperparameters
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        dbscan.fit(scaled_features)

        # Calculate Silhouette Score
        silhouette_dbscan = silhouette_score(scaled_features, dbscan.labels_)

        print(f"Parameters: eps={eps}, min_samples={min_samples}")
        print(f"Silhouette Score: {silhouette_dbscan:.2f}\n")

        # Update best parameters if current Silhouette Score is higher
        if silhouette_dbscan > best_silhouette_score:
            best_eps = eps
            best_min_samples = min_samples
            best_silhouette_score = silhouette_dbscan

print("Best Parameters:")
print(f"eps={best_eps}, min_samples={best_min_samples}")
print(f"Best Silhouette Score: {best_silhouette_score:.2f}")



eps_values = np.arange(0.1, 1.0, 0.1)
min_samples_values = [3, 5, 10, 20]

best_eps = None
best_min_samples = None
best_davies_bouldin_score = float('inf')

for eps in eps_values:
    for min_samples in min_samples_values:
        # Fit DBSCAN with current hyperparameters
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        dbscan.fit(scaled_features)

        # Calculate Davies-Bouldin Index
        davies_bouldin_dbscan = davies_bouldin_score(scaled_features, dbscan.labels_)


        print(f"Parameters: eps={eps}, min_samples={min_samples}")
        print(f"Davies-Bouldin Index: {davies_bouldin_dbscan:.2f}\n")

        # Update best parameters if current Davies-Bouldin Index is lower
        if davies_bouldin_dbscan < best_davies_bouldin_score:
            best_eps = eps
            best_min_samples = min_samples
            best_davies_bouldin_score = davies_bouldin_dbscan

print("Best Parameters:")
print(f"eps={best_eps}, min_samples={best_min_samples}")
print(f"Best Davies-Bouldin Index: {best_davies_bouldin_score:.2f}")


eps = 0.70
min_samples = 3
dbscan = DBSCAN(eps=eps, min_samples=min_samples)
dbscan.fit(scaled_features)

G = nx.Graph()
labels = dbscan.labels_
anomaly_nodes = [node for node, label in enumerate(labels) if label == -1]
for node in range(len(labels)):
    G.add_node(node, color='r' if node in anomaly_nodes else 'b')

pos = nx.spring_layout(G)
node_colors = [data['color'] for node, data in G.nodes(data=True)]

outlier_indices = df[labels == -1].index

anomalies = df.iloc[outlier_indices]
print("Anomalies:")
print(anomalies)


df['dbscan_cluster'] = labels

# Now calculate silhouette score and Davies-Bouldin index based on DBSCAN labels
silhouette_dbscan = silhouette_score(scaled_features, df['dbscan_cluster'])
davies_bouldin_dbscan = davies_bouldin_score(scaled_features, df['dbscan_cluster'])

print(f"Silhouette Score (DBSCAN): {silhouette_dbscan:.2f}")
print(f"Davies-Bouldin Index (DBSCAN): {davies_bouldin_dbscan:.2f}")




pos = nx.spring_layout(G)
node_colors = [data['color'] for node, data in G.nodes(data=True)]

plt.scatter(features[feature_columns[0]], features[feature_columns[1]], c=labels)
plt.title('DBSCAN Anomaly Detection')
plt.xlabel(feature_columns[0])
plt.ylabel(feature_columns[1])
plt.show()

plt.hist(labels, bins=50, color='blue', edgecolor='black')
plt.title('Anomaly Distribution')
plt.xlabel('Cluster Label')
plt.ylabel('Frequency')
plt.show()

outlier_indices = df[labels == -1].index




#Isolation Forest



df = pd.read_csv('eth-dataset.csv')

feature_columns = ['value', 'gas', 'gas_price']
features = df[feature_columns]

contamination = 0.05
isolation_forest = IsolationForest(contamination=contamination)
isolation_forest.fit(features)

outliers = isolation_forest.predict(features)
plt.scatter(features[feature_columns[0]], features[feature_columns[1]], c=outliers, cmap='viridis')
plt.title('Isolation Forest Anomaly Detection (Scatter Plot)')
plt.xlabel(feature_columns[0])
plt.ylabel(feature_columns[1])
plt.colorbar()
plt.show()




outlier_indices = [i for i, outlier in enumerate(outliers) if outlier == -1]

df.iloc[outlier_indices]














